#!/usr/bin/env python

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback
import tensorflow as tf
import numpy as np
from utils import kronecker_product, SparseIndices
from scipy import sparse



import pandas as pd

from sklearn import tree

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)



# The function to execute the training.
def train():
    print('Starting the training.')
    try:        
        #########################################
        #constants
        
        datatype = tf.complex128
        
        with open(param_path, 'r') as tc:
             trainingParams = json.load(tc)
        N = int(trainingParams.get('N', None))  
        steps = int(trainingParams.get('steps', None))

        dim = 2**(N+1) # Dimension of Hilbert space of N nuclear and 1 electronic spin

        Amean = 1
        gamma = 1
        #g = np.random.randn(N) # coupling strengts
        g = np.ones(N)
        g=g/(np.sqrt((g**2).sum()));
        A = Amean/g.sum()
        I_ind = np.real(A**2/(gamma+1j*Amean))*2/N

        gammaT = tf.constant(gamma, dtype=datatype)
        AT = tf.constant(A, dtype=datatype)
        eps = tf.placeholder(dtype=datatype, shape=())

        #########################################
        #pauliMatrices

        sigmaz = tf.constant([[1/2, 0], [0, -1/2]], dtype= datatype)
        splus  = tf.constant([[0, 1], [0, 0]], dtype = datatype)
        sminus = tf.constant([[0, 0], [1, 0]], dtype = datatype)


        #########################################
        #electron Spin operators (QD)

        Sm = kronecker_product(sminus, tf.eye(int(dim/2), dtype = datatype))
        Sp = tf.transpose(Sm)
        Sz = kronecker_product(sigmaz, tf.eye(int(dim/2), dtype = datatype))


        Am = tf.zeros(int(dim/2), dtype = datatype)
        Iz = tf.zeros(int(dim/2), dtype = datatype)
        for p in range(N): #creates the collective (inhomo) spin operators
            Am = Am+g[p]*kronecker_product(kronecker_product(tf.eye(2**p, dtype = datatype),sminus),tf.eye(2**(N-1-p), dtype = datatype))
            Iz = Iz+kronecker_product(kronecker_product(tf.eye(2**p, dtype = datatype),sigmaz),tf.eye(2**(N-1-p), dtype = datatype))

        Am = kronecker_product(tf.eye(2, dtype = datatype),Am)
        Ap = tf.transpose(Am)
        Iz = kronecker_product(tf.eye(2, dtype = datatype),Iz)



            
        #########################################
        #Initial state
        print(os.listdir(training_path))
            
        sparse_rho = pickle.load(open(os.path.join(training_path, 'init.pckl'), "rb"))

        rho_ind = SparseIndices(sparse_rho)
        rho2 = tf.sparse_to_dense(rho_ind['indices'], rho_ind['dense_shape'], rho_ind['values']) # Hack because sparse_to_dense not implemented for complex
        rho1 = tf.complex(rho2,tf.zeros(rho_ind['dense_shape'], dtype=tf.float64))
        rho = tf.Variable(rho1, dtype=datatype)  


        ########################################
        #Operations
        H = 0.5 * AT * (tf.matmul(Sp,Am) + tf.matmul(Sm,Ap))
        IL = gammaT*(tf.matmul(tf.matmul(Sm,rho),Sp)
                - 0.5 * (tf.matmul(tf.matmul(Sp,Sm),rho) + tf.matmul(rho, tf.matmul(Sp,Sm))))\
         - 1j * (tf.matmul(H,rho) - tf.matmul(rho, H))
            
        rho_ = rho + eps * IL

        step = rho.assign(rho_)
        
        pol = tf.real(tf.trace(tf.matmul(rho, Iz)))

        global_step = tf.train.get_or_create_global_step()
        increment_global_step_op = tf.assign(global_step, global_step+1)
        


        polarization = np.zeros(steps)
        init = tf.global_variables_initializer()
        with tf.Session() as sess:
            sess.run(init)
            
            for i in range(steps):

                _ , polarization[i] = sess.run([step, pol], feed_dict={eps:0.3})
        intensity = -np.diff(polarization)
        out= {'intensity': intensity, 'I_ind': I_ind}
        
        pickle.dump(out, open(os.path.join(model_path, 'out.pckl'), "wb"))
        #np.save(os.path.join(model_path, 'intensity.npy'),intensity)
        print('Training complete.')
        
    
    
#         # Read in any hyperparameters that the user passed with the training job
#         with open(param_path, 'r') as tc:
#             trainingParams = json.load(tc)

#         # Take the set of files and read them all into a single pandas dataframe
#         input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
#         if len(input_files) == 0:
#             raise ValueError(('There are no files in {}.\n' +
#                               'This usually indicates that the channel ({}) was incorrectly specified,\n' +
#                               'the data specification in S3 was incorrectly specified or the role specified\n' +
#                               'does not have permission to access the data.').format(training_path, channel_name))
#         raw_data = [ pd.read_csv(file, header=None) for file in input_files ]
#         train_data = pd.concat(raw_data)

#         # labels are in the first column
#         train_y = train_data.ix[:,0]
#         train_X = train_data.ix[:,1:]

#         # Here we only support a single hyperparameter. Note that hyperparameters are always passed in as
#         # strings, so we need to do any necessary conversions.
#         max_leaf_nodes = trainingParams.get('max_leaf_nodes', None)
#         if max_leaf_nodes is not None:
#             max_leaf_nodes = int(max_leaf_nodes)

#         # Now use scikit-learn's decision tree classifier to train the model.
#         clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)
#         clf = clf.fit(train_X, train_y)

#         # save the model
#         with open(os.path.join(model_path, 'decision-tree-model.pkl'), 'w') as out:
#             pickle.dump(clf, out)
#         print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
